# 分类和预测

## 定义

分类：用于预测数据对象的分类标号。

预测：用于预测数据对象的**连续值或者有序值**

## 使用分类步骤

### 第一步：学习建模

（1）建立分类模型：利用分类算法对训练集进行训练，得到分类模型，这就是**“有指导的学习”**。

***

分类的是实现——利用模型预测

分类模型的效果要进行评判，需要把数据分层训练集和测试集，**modeler中就是分区**。一般测试集在30%左右。

![1555325926368](TyporaImg/1555325926368.png)

### 第二步：使用模型进行分类

（2）使用模型进行分类：

- 测试数据集：用于评估模型的预测准确率。
  - 测试集要独立于训练样本集，否则就会出现“过分适应数据”的情况
  - 对每个测试样本，将已知的类标号和该样本的学习模型类预测比较，模型在给定测试集上的准确理是正确被模型分类的测试样本百分比。
- 如认为模型准确率可以接收，就可以用它来对类标号未知的数据元组进行分类。

***

> ​	**分类的核心是归纳**，归纳是从特殊到一般的过程。归纳推理从若干个事实表征出的特征中，通过比较、总结、概括而得出的一个规律性结论。

![1555327345983](TyporaImg/1555327345983.png)

### 分类问题的一般化方法

归纳学习存在基本假设：

* 假设如果能够在足够大的训练样本集中很好地逼近目标函数，则它也能在未见样本中很好地逼近目标函数。
* 该假定是归纳学习有效性的前提条件

## 分类性能的度量

***

> ​	评价分类性能的好坏，需要采取一定的评价指标和准则

### 查全率、查准率和F1

* **对于二分类问题**，可将样例根据真实类别与机器学习预测类别的组合划分为**真正例TP（True positive）、假正例FP（false positive）、真反例TN(TRUE NEGATIVE)、假反例FN（FALSE NEGATIVE）**这四种情况
* 分类结果可以用混淆矩阵表示

![1555327826829](TyporaImg/1555327826829.png)

* 查准率、查全率和F1

* 查准率![1555327872570](TyporaImg/1555327872570.png)

  > ​	查全率就是实际和预测相同的，除以所有

* 查全率![1555327940143](TyporaImg/1555327940143.png)

  > ​	查全率也叫做召回率，所有正例中（这里的正例是指实际上是正确的），正确返回了多少

**查准率和查全率是一对矛盾的度量，一般查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低**

***

F1的度量：![1555328525897](TyporaImg/1555328525897.png)，**F2是查准率和查全率的调和平均**

> ​	如果有时候在一些应用中，对查准率和查全率的重视程度不同，是可以加权重的

![1555328594600](TyporaImg/1555328594600.png)

β>1查全率有更大影响，β<1查准率有更大影响

***

对于滴多分类问题：

![1555328671670](TyporaImg/1555328671670.png)

### ROC曲线

很多机器学习为测试样本产生一个**实值或者预测概率**，然后大于阈值分为正，小于则为反。然后对结果样例进行排序，最可能的正例方最前面，最不可能的放后面，按照顺序逐个把样本作为正例进行预测，每次都要计算两个重要值，**FPR和TPR**,就可以得出ROC曲线。

> TPR=TP/(TP/FN)
>
> FPR=FP/(FP+TN)

![1555328951918](TyporaImg/1555328951918.png)

这个图明显可以看到如果曲线更加上面，预测就是更加准确，**比较合理比较就是利用曲线和X轴的面积衡量，越大就越好**

***



## 决策树

决策树是一种简单且应用广泛的预测方法

决策树又称为判定树，是运用分类的一种树结构，其中每个内部节点表示一次测试，每条边代表一个测试结果，**叶节点**代表某个**类**或者**类的分布**，最上面的是**根节点**

![1555329310548](TyporaImg/1555329310548.png)

## 决策数生成

决策树生成有两个阶段：

* 决策树构建
  * 开始时，所有的训练样本都·在根节点
  * 递归的通过选定的属性，来划分样本（必须是离散值）
* 树剪枝
  * 许多分枝反映的是训练数据中的噪声和孤立点，树剪枝试图剪去这种分歧

## 决策树可以用于两个目的

* 描述性建模：分类模型可以作为解释工具，用于区分不同类中的对象；
* 预测性建模：分类模型还可以用于预测未知记录的类标号

## 决策树的特点

* 决策树体现了对样本数据不断分组的过程。每个节点都是具有一定样本量的样本，**根节点的样本量最大，其他节点的样本量依层递减**。
* 决策树分为分类树和回归树。对于分类树，叶节点的预测值是该样本输出变量值的雷贝；对于回归树，叶节点的预测值是该样本输出变量值的平均值。
* 决策树体现了输入变量和输出变量的逻辑关系。类似于IF-ELSE关系这种。

## 决策树的核心问题

决策树归纳的学习算法必须解决两个核心问题之一：

* 如何**从众多的输入变量中选择一个当最佳的分组变量**

* 如何**从分组变量的众多取值中找到一个最佳分割点**

  > * **决策树生成的核心算法是确定决策树的分支准则，即选择某个属性作为测试条件**
  >
  > *	决策树的剪枝：决策树可能出现过拟合现象，（过拟合是指因失去一般代表性而无法用于对新数据的分类预测，这种现象叫做“过拟合”）

## 决策树算法

### CLS算法

### ID3算法

ID3算法针对**属性选择问题**，ID3算法是决策树学习方法中最具影响和典型的算法。

这个方法用到了**信息增益**选择测试属性。

当我们获取信息时候，可以将不确定的内容转为确定的内容，但**获取信息有用和无用之分或者说有重要程度之分。选择属性的时候，进行要样本纯**

***

#### 信息熵

![1555330268248](TyporaImg/1555330268248.png)

![1555330184207](TyporaImg/1555330184207.png)



P1,P2,...P32分别是这32个求对获胜的概率

![1555330309786](TyporaImg/1555330309786.png)

#### 计算信息增益

* 信息增益被定义为**原始的分割的熵**与**按某一属性划分后各个分割的熵的累加得到的总熵**之间的**差**
* 信息增益是划分前后进行正确预测苏哦需要信息量之差。
* 选择具有**最高信息增益的属性**作为当前节点的测试属性。

#### 基尼（Gini)指数

![1555330677087](TyporaImg/1555330677087.png)

![1555330694897](TyporaImg/1555330694897.png)

#### 误分类错误

![1555330725532](TyporaImg/1555330725532.png)

![1555330732519](TyporaImg/1555330732519.png)