# 分类和预测

## 定义

分类：用于预测数据对象的分类标号。

预测：用于预测数据对象的**连续值或者有序值**

## 使用分类步骤

### 第一步：学习建模

（1）建立分类模型：利用分类算法对训练集进行训练，得到分类模型，这就是**“有指导的学习”**。

***

分类的是实现——利用模型预测

分类模型的效果要进行评判，需要把数据分层训练集和测试集，**modeler中就是分区**。一般测试集在30%左右。

![1555325926368](TyporaImg/1555325926368.png)

### 第二步：使用模型进行分类

（2）使用模型进行分类：

- 测试数据集：用于评估模型的预测准确率。
  - 测试集要独立于训练样本集，否则就会出现“过分适应数据”的情况
  - 对每个测试样本，将已知的类标号和该样本的学习模型类预测比较，模型在给定测试集上的准确理是正确被模型分类的测试样本百分比。
- 如认为模型准确率可以接收，就可以用它来对类标号未知的数据元组进行分类。

***

> ​	**分类的核心是归纳**，归纳是从特殊到一般的过程。归纳推理从若干个事实表征出的特征中，通过比较、总结、概括而得出的一个规律性结论。

![1555327345983](TyporaImg/1555327345983.png)

### 分类问题的一般化方法

归纳学习存在基本假设：

* 假设如果能够在足够大的训练样本集中很好地逼近目标函数，则它也能在未见样本中很好地逼近目标函数。
* 该假定是归纳学习有效性的前提条件

## 分类性能的度量

***

> ​	评价分类性能的好坏，需要采取一定的评价指标和准则

### 查全率、查准率和F1

* **对于二分类问题**，可将样例根据真实类别与机器学习预测类别的组合划分为**真正例TP（True positive）、假正例FP（false positive）、真反例TN(TRUE NEGATIVE)、假反例FN（FALSE NEGATIVE）**这四种情况
* 分类结果可以用混淆矩阵表示

![1555327826829](TyporaImg/1555327826829.png)

* 查准率、查全率和F1

* 查准率![1555327872570](TyporaImg/1555327872570.png)

  > ​	查全率就是实际和预测相同的，除以所有

* 查全率![1555327940143](TyporaImg/1555327940143.png)

  > ​	查全率也叫做召回率，所有正例中（这里的正例是指实际上是正确的），正确返回了多少

**查准率和查全率是一对矛盾的度量，一般查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低**

***

F1的度量：![1555328525897](TyporaImg/1555328525897.png)，**F2是查准率和查全率的调和平均**

> ​	如果有时候在一些应用中，对查准率和查全率的重视程度不同，是可以加权重的

![1555328594600](TyporaImg/1555328594600.png)

β>1查全率有更大影响，β<1查准率有更大影响

***

对于滴多分类问题：

![1555328671670](TyporaImg/1555328671670.png)

### ROC曲线

很多机器学习为测试样本产生一个**实值或者预测概率**，然后大于阈值分为正，小于则为反。然后对结果样例进行排序，最可能的正例方最前面，最不可能的放后面，按照顺序逐个把样本作为正例进行预测，每次都要计算两个重要值，**FPR和TPR**,就可以得出ROC曲线。

> TPR=TP/(TP/FN)
>
> FPR=FP/(FP+TN)

![1555328951918](TyporaImg/1555328951918.png)

这个图明显可以看到如果曲线更加上面，预测就是更加准确，**比较合理比较就是利用曲线和X轴的面积衡量，越大就越好**

***



## 决策树

决策树是一种简单且应用广泛的预测方法

决策树又称为判定树，是运用分类的一种树结构，其中每个内部节点表示一次测试，每条边代表一个测试结果，**叶节点**代表某个**类**或者**类的分布**，最上面的是**根节点**

![1555329310548](TyporaImg/1555329310548.png)

## 决策数生成

决策树生成有两个阶段：

* 决策树构建
  * 开始时，所有的训练样本都·在根节点
  * 递归的通过选定的属性，来划分样本（必须是离散值）
* 树剪枝
  * 许多分枝反映的是训练数据中的噪声和孤立点，树剪枝试图剪去这种分歧

## 决策树可以用于两个目的

* 描述性建模：分类模型可以作为解释工具，用于区分不同类中的对象；
* 预测性建模：分类模型还可以用于预测未知记录的类标号

## 决策树的特点

* 决策树体现了对样本数据不断分组的过程。每个节点都是具有一定样本量的样本，**根节点的样本量最大，其他节点的样本量依层递减**。
* 决策树分为分类树和回归树。对于分类树，叶节点的预测值是该样本输出变量值的雷贝；对于回归树，叶节点的预测值是该样本输出变量值的平均值。
* 决策树体现了输入变量和输出变量的逻辑关系。类似于IF-ELSE关系这种。

## 决策树的核心问题

决策树归纳的学习算法必须解决两个核心问题之一：

* 如何**从众多的输入变量中选择一个当最佳的分组变量**

* 如何**从分组变量的众多取值中找到一个最佳分割点**

  > * **决策树生成的核心算法是确定决策树的分支准则，即选择某个属性作为测试条件**
  >
  > *	决策树的剪枝：决策树可能出现过拟合现象，（过拟合是指因失去一般代表性而无法用于对新数据的分类预测，这种现象叫做“过拟合”）

## 决策树算法

> ​	**注意，下面各种纯度的算法，都是针对一个数据集，因为一般分类会分成多个数据集，各种增益，都要用初始增益减去多个数据集的纯度的加权平均**

### CLS算法

### ID3算法

ID3算法针对**属性选择问题**，ID3算法是决策树学习方法中最具影响和典型的算法。

这个方法用到了**信息增益**选择测试属性。

当我们获取信息时候，可以将不确定的内容转为确定的内容，但**获取信息有用和无用之分或者说有重要程度之分。选择属性的时候，进行要样本纯**

***

#### 信息熵

![1555330268248](TyporaImg/1555330268248.png)

![1555330184207](TyporaImg/1555330184207.png)



P1,P2,...P32分别是这32个求对获胜的概率

![1555330309786](TyporaImg/1555330309786.png)

#### 计算信息增益

* 信息增益被定义为**原始的分割的熵**与**按某一属性划分后各个分割的熵的累加得到的总熵**之间的**差**
* 信息增益是划分前后进行正确预测苏哦需要信息量之差。
* 选择具有**最高信息增益的属性**作为当前节点的测试属性。

#### 基尼（Gini)指数

![1555330677087](TyporaImg/1555330677087.png)

![1555330694897](TyporaImg/1555330694897.png)

#### 误分类错误

![1555330725532](TyporaImg/1555330725532.png)

![1555330732519](TyporaImg/1555330732519.png)

### 决策树算法的探讨

#### 冗余和不相关属性

* 一个属性如果和另一个属性强相关，那么他就是冗余的。在==两个冗余属性中，如果已经选择一个作为用于划分的属性，则另一个将被忽略，冗余属性不会对决策树的精准性有影响。==
* 如果数据集中有很多与目标属性不相关的属性（即对分类任务没有用的属性），则某些不相关的属性可能在树的构造过程中偶然被选中，==导致决策树过于庞大==。
* ==构建决策树分类模型时，在预处理阶段需要删除不相关的属性==，冗余属性的保留使得进入分裂属性的选择具有随机性。

#### ID3算法小结

> ​	ID3算法的基本思想：以**信息熵**为度量，用于决策树节点的属性选择，每次优先选取**信息增益**最多的属性，==以构造一棵熵值下降最快的决策树==，到叶子节点处的熵值为0.

- 基本的决策树生成算法是一个**贪心算法**，采取==自上而下、分而治之==的递归方式来构造。
- 决策树是一种自顶向下增长树的==贪心算法==，==在每个节点它都希望选取的是当前最好分类样本的属性==，继续这个过程直到这棵树能完美地分类训练集，或所有的属性已经被用过。
- 算法的重点是**属性的选取**
- 贪心算法的缺点是**局部最优**。

#### 对于二类问题：

![1555985881762](TyporaImg/1555985881762.png)

> ​	横坐标为属于其中一个类的概率

三种方法都在类分布均衡时（即当p=0.5)达到最大值，而当所有记录度属于同一个类（p=0或p=1)时达到最小值。

#### 如何选择属性测试条件

***

取决于属性类型：

* 标称
* 有序
* 连续

***

取决于分裂的“路”数：

* 二路分裂
* 多路分裂

***

##### 基于**标称属性**的分裂：

* 多路分裂：有多少不同的属性，就划分为多少个子集。

  ![1555986352123](TyporaImg/1555986352123.png)

* 二路分裂：将记录集分为两个子集需要找到最佳的划分。

  ![1555986411724](TyporaImg/1555986411724.png)

对于**有序属性**的分裂，与标称属性类似，但是要主义，不能违背==有序性==

##### 连续属性的分裂

> ​	连续属性的分裂就是将连续属性离散化，然后分裂。

#### 不纯度度量

熵和Gini指标等的不纯性度量**倾向于选择产生大量小而纯的子集的分裂方案。**

> ​	缺点：考虑如果按照员工ID来分裂，可能会产生更纯的划分，但明显顾客的ID不是一个有预测性的属性。
>
> 解决方案：
>
> * 限制测试条件只能二元划分（如CART)
> * 修改评估划分的标准，==把属性测试条件产生的**输出数**也考虑进去==（如C4.5)

#### ID3到C4.5

> ​	ID3算法解决了CLS算法中的属性选择问题，以**信息增益**为度量，用于决策树节点的属性选择。==信息增益倾向于选择产生大量小而纯的子集的分裂方案==。
>
> 而且有极端情况，如果用ID来分，那肯定是纯的，分很多类。
>
> C4.5算法对此进行改进，用**信息增益率**（用信息增益除以一个变量，这个变量随着分裂路数增大而增大）代替**信息增益**来选择属性。

#### C4.5算法

##### 对于ID3的改进

* 用==信息增益率==代替信息增益来选择属性

* 能够完成对==连续属性==的==离散化==处理。

* 能处理==属性缺失==的情况

* 在决策树构造完成后进行==剪枝==

  > ​	ID3是决策树的鼻祖算法，ID3改进成C4.5 ，C5.0仅仅是C4.5的商业化版本，基本相同。

##### 增益率

![1555987907077](TyporaImg/1555987907077.png)

![1555987917522](TyporaImg/1555987917522.png)

> ​	n~i~是其其中i部分记录的个数,k就是父节点分裂K个部分

* 选择有==最大信息增益率==的属性作为分裂属性
* SplitINFO的调节，使得具有高熵值的分裂方法，产生小而纯的子集方法，受到处罚。

##### 连续属性与分裂点

#### CART算法

> ​	CART(Classification and Regression Tree )算法是一种既可以处理离散变量，也可以处理连续变量的决策树。

##### CART与C4.5的差别

* 既可以处理分类型，也可以处理数值型
* 只能建立==二叉树==
* C4.5用的是信息增益率，而CART用的是==Gini系数和方差==
* C4.5算法使用**后修剪**方式，依据训练样本进行修建；CART算法使用==预修剪和后修剪==相结合的修剪方式，对测试样本集进行修剪。

##### 离散数据的处理

​	因为CART只能使用二叉树的，对于多分类，首先要将多类合并成两个类别，形成超类；

##### 数值型属性处理

###### 对于分类树

​	首先，将数据按照升序排列，然后从大到小依次以相邻数值的中间值作为组限，将样本划分两组，计算两者之间的白能量输出的差异性（其实就是Gini的差异）。按反复计算就可以得到最佳分类。

###### 对于回归树



#### 决策树总结

优点：

![1555989386051](TyporaImg/1555989386051.png)

缺点：

* 数据碎片：因为到了下面，叶子节点记录会越来越少，不具有代表性。解决方法：==党样本数小于某个特定阈值就停止分裂==
* 重复：
* 限制连续属性之间复杂关系的建模的表达能力：每次都只能取一个属性，有些时候，东西是由两个或者两个以上属性来决定的，比如DRUG中有Na/K.

### 决策树中的问题及解决

***

- 欠拟合和过拟合
- 缺失值
- 分类成本

#### 决策树停止准则

- 当一个节点处==所有的记录属于同一类==，停止
- 当一个节点==所有记录有相似的属性值==时，停止

​	一个好的分类模型必须具有较低的训练误差和低泛化误差

- ==训练误差==：训练记录上误分类样本比例
- ==泛化误差==：模型在未知记录上的期望误差

#### 过拟合和欠拟合

##### 过拟合

随着决策树生长，处理样本不断减少，其对==数据总体规律代表程度下降==。可能是噪声引起的过拟合

解决方法：

* 预修剪：及早停止生长

  * 常见中值条件：
    * 所有的实例是同一类
    * 所有的属性值相同
  * 严格的条件：
    * 给定决策树的高度
    * 设定实例个数小于阈值

* 后修剪：

  * 修剪后==泛化错误==有所改进，则用一个叶节点替换子树

  * 用子树中最常使用的分支代替子树（就是结合业务）

    > ​	与先剪枝相比，==后剪枝技术产生结果更好==

### Modeler中决策树算法模型比较

* 选择分类模型的时候，首先要确定输出目标是什么，==输出目标标称型还是连续型==
* 标称型的用C5.0，连续性的用C&R和CHAID
* 判断是==二元还是多元的==
  * 二元的用C&R和QUEST
  * 多元的用C5.0和CHAID
* 决策树算法的核心是==决策树的分支准则==。五种算法的分支准则不同：
  * C5.0使用信息增益率
  * C&R使用基尼系数
  * QUEST使用卡方检验和方差检验
  * CHAID使用卡方检验
  * 决策列表使用统计置信度

#### Bagging技术和Boosting技术

* 与Bagging技术类似，还有另外一种技术是Boosting技术，C5.0、CHAID、QUEST、C&R，